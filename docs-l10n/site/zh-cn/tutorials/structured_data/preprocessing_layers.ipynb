{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg02FZzDyEqd"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2mapZ9afGJ69"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMYQvJuBi7MS"
      },
      "source": [
        "# 使用 Keras 预处理层对结构化数据进行分类"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FaL4wnr22oy"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://tensorflow.google.cn/tutorials/structured_data/preprocessing_layers\"><img src=\"https://tensorflow.google.cn/images/tf_logo_32px.png\">在 TensorFlow.org 上查看</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/tutorials/structured_data/preprocessing_layers.ipynb\"><img src=\"https://tensorflow.google.cn/images/colab_logo_32px.png\">在 Google Colab 中运行</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/tutorials/structured_data/preprocessing_layers.ipynb\"><img src=\"https://tensorflow.google.cn/images/GitHub-Mark-32px.png\">在 GitHub 上查看源代码</a></td>\n",
        "  <td> <img><a>下载笔记本</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nna1tOKxyEqe"
      },
      "source": [
        "本教程演示了如何对结构化数据（例如 CSV 中的表格数据）进行分类。您将使用 [Keras](https://tensorflow.google.cn/guide/keras) 定义模型，并使用[预处理层](https://tensorflow.google.cn/guide/keras/preprocessing_layers)作为桥梁，将 CSV 中的列映射到用于训练模型的特征。本教程包含以下操作的完整代码：\n",
        "\n",
        "- 使用 [Pandas](https://pandas.pydata.org/) 加载 CSV 文件。\n",
        "- 构建输入流水线以使用 [tf.data](https://tensorflow.google.cn/guide/datasets) 对行进行批处理和乱序。\n",
        "- 使用 Keras 预处理层将 CSV 中的列映射到用于训练模型的特征。\n",
        "- 使用 Keras 构建、训练和评估模型。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5xkXCicjFQD"
      },
      "source": [
        "注：本教程类似于[使用特征列对结构化数据进行分类](https://tensorflow.google.cn/tutorials/structured_data/feature_columns)。此版本使用新的实验性 Keras [预处理层](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/experimental/preprocessing)而不是 `tf.feature_column`。Keras 预处理层更直观，可以轻松包含在模型中以简化部署。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHxU1FMNpomc"
      },
      "source": [
        "## 数据集\n",
        "\n",
        "您将使用 PetFinder [数据集](https://www.kaggle.com/c/petfinder-adoption-prediction)的简化版本。CSV 中有几千行。每行描述一个宠物，每列描述一个特性。您将使用此信息来预测宠物是否会被领养。\n",
        "\n",
        "以下是对该数据集的描述。请注意，其中既有数值列，也有分类列。还有一个您不会在本教程中用到的自由文本列。\n",
        "\n",
        "列 | 描述 | 特征类型 | 数据类型\n",
        "--- | --- | --- | ---\n",
        "Type | 动物类型（狗、猫） | 分类 | 字符串\n",
        "Age | 宠物年龄 | 数值 | 整数\n",
        "Breed1 | 宠物的主要品种 | 分类 | 字符串\n",
        "Color1 | 宠物的颜色 1 | 分类 | 字符串\n",
        "Color2 | 宠物的颜色 2 | 分类 | 字符串\n",
        "MaturitySize | 成年个体大小 | 分类 | 字符串\n",
        "FurLength | 毛发长度 | 分类 | 字符串\n",
        "Vaccinated | 宠物已接种疫苗 | 分类 | 字符串\n",
        "Sterilized | 宠物已绝育 | 分类 | 字符串\n",
        "Health | 健康状况 | 分类 | 字符串\n",
        "Fee | 领养费 | 数值 | 整数\n",
        "Description | 关于此宠物的简介 | 文本 | 字符串\n",
        "PhotoAmt | 为该宠物上传的照片总数 | 数值 | 整数\n",
        "AdoptionSpeed | 领养速度 | 分类 | 整数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjFbdBldyEqf"
      },
      "source": [
        "## 导入TensorFlow和其他库\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_BdyQlPjfDW"
      },
      "outputs": [],
      "source": [
        "!pip install -q sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LklnLlt6yEqf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKU7RyoQGVKB"
      },
      "outputs": [],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXvBvobayEqi"
      },
      "source": [
        "## 使用 Pandas 创建数据帧\n",
        "\n",
        "[Pandas](https://pandas.pydata.org/) 是一个 Python 库，其中包含许多有用的加载和处理结构化数据的实用工具。您将使用 Pandas 从 URL 下载数据集，并将其加载到数据帧中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ4Ajn-YyEqj"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "dataset_url = 'http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip'\n",
        "csv_file = 'datasets/petfinder-mini/petfinder-mini.csv'\n",
        "\n",
        "tf.keras.utils.get_file('petfinder_mini.zip', dataset_url,\n",
        "                        extract=True, cache_dir='.')\n",
        "dataframe = pd.read_csv(csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uiq4hoIGyXI"
      },
      "outputs": [],
      "source": [
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3zDbrozyEqq"
      },
      "source": [
        "## 创建目标变量\n",
        "\n",
        "Kaggle 比赛中的任务是预测宠物被领养的速度（例如，在第一周、第一个月、前三个月等）。我们针对教程进行一下简化。在这里，您将把它转化为一个二元分类问题，并简单地预测宠物是否被领养。\n",
        "\n",
        "修改标签列后，0 表示宠物未被领养，1 表示宠物已被领养。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmMDc46-yEqq"
      },
      "outputs": [],
      "source": [
        "# In the original dataset \"4\" indicates the pet was not adopted.\n",
        "dataframe['target'] = np.where(dataframe['AdoptionSpeed']==4, 0, 1)\n",
        "\n",
        "# Drop un-used columns.\n",
        "dataframe = dataframe.drop(columns=['AdoptionSpeed', 'Description'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp0NCbswyEqs"
      },
      "source": [
        "## 将数据帧拆分为训练集、验证集和测试集\n",
        "\n",
        "您下载的数据集是单个 CSV 文件。您将把它拆分为训练集、验证集和测试集。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT6HdyEwyEqt"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(dataframe, test_size=0.2)\n",
        "train, val = train_test_split(train, test_size=0.2)\n",
        "print(len(train), 'train examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_7uVu-xyEqv"
      },
      "source": [
        "## 使用 tf.data 创建输入流水线\n",
        "\n",
        "接下来，您将使用 [tf.data](https://tensorflow.google.cn/guide/datasets) 封装数据帧，以便对数据进行乱序和批处理。如果您处理的 CSV 文件非常大（大到无法放入内存），则可以使用 tf.data 直接从磁盘读取文件。本教程中没有涉及这方面的内容。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r4j-1lRyEqw"
      },
      "outputs": [],
      "source": [
        "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('target')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(batch_size)\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYxIXH579uS9"
      },
      "source": [
        "现在您已经创建了输入流水线，我们调用它来查看它返回的数据的格式。您使用了小批次来保持输出的可读性。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYiNH-QI96Jo"
      },
      "outputs": [],
      "source": [
        "batch_size = 5\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFYir6S8HgIJ"
      },
      "outputs": [],
      "source": [
        "[(train_features, label_batch)] = train_ds.take(1)\n",
        "print('Every feature:', list(train_features.keys()))\n",
        "print('A batch of ages:', train_features['Age'])\n",
        "print('A batch of targets:', label_batch )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geqHWW54Hmte"
      },
      "source": [
        "您可以看到数据集（从数据帧）返回了一个列名称字典，该字典映射到来自数据帧中行的列值。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v50jBIuj4gb"
      },
      "source": [
        "## 演示预处理层的使用。\n",
        "\n",
        "Keras 预处理层 API 允许您构建 Keras 原生输入处理流水线。您将使用 3 个预处理层来演示特征预处理代码。\n",
        "\n",
        "- [`Normalization`](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/experimental/preprocessing/Normalization) - 数据的特征归一化。\n",
        "- [`Normalization`](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/experimental/preprocessing/CategoryEncoding) - 类别编码层。\n",
        "- [`StringLookup`](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/experimental/preprocessing/StringLookup) - 将字符串从词汇表映射到整数索引。\n",
        "- [`IntegerLookup`](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/experimental/preprocessing/IntegerLookup) - 将词汇表中的整数映射到整数索引。\n",
        "\n",
        "您可以在[此处](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/experimental/preprocessing)找到可用预处理层的列表。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twXBSxnT66o8"
      },
      "source": [
        "### 数值列\n",
        "\n",
        "对于每个数值特征，您将使用 Normalization() 层来确保每个特征的平均值为 0，且其标准差为 1。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OosUh4kTsK_q"
      },
      "source": [
        "`get_normalization_layer` 函数返回一个层，该层将特征归一化应用于数值特征。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6OuEKMMyEq1"
      },
      "outputs": [],
      "source": [
        "def get_normalization_layer(name, dataset):\n",
        "  # Create a Normalization layer for our feature.\n",
        "  normalizer = preprocessing.Normalization(axis=None)\n",
        "\n",
        "  # Prepare a Dataset that only yields our feature.\n",
        "  feature_ds = dataset.map(lambda x, y: x[name])\n",
        "\n",
        "  # Learn the statistics of the data.\n",
        "  normalizer.adapt(feature_ds)\n",
        "\n",
        "  return normalizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpKgUDyk69bM"
      },
      "outputs": [],
      "source": [
        "photo_count_col = train_features['PhotoAmt']\n",
        "layer = get_normalization_layer('PhotoAmt', train_ds)\n",
        "layer(photo_count_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foWY00YBUx9N"
      },
      "source": [
        "注：如果您有许多数值特征（数百个或更多），首先将它们连接起来并使用单个 [normalization](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/experimental/preprocessing/Normalization) 层会更有效。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVD--2WZ7vmh"
      },
      "source": [
        "### 分类列\n",
        "\n",
        "在此数据集中，Type 表示为字符串（例如 'Dog' 或 'Cat'）。您不能将字符串直接馈送给模型。预处理层负责将字符串表示为独热向量。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWlkOPwMsxdv"
      },
      "source": [
        "`get_category_encoding_layer` 函数返回一个层，该层将值从词汇表映射到整数索引，并对特征进行独热编码。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmgaeRjlDoUO"
      },
      "outputs": [],
      "source": [
        "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
        "  # Create a StringLookup layer which will turn strings into integer indices\n",
        "  if dtype == 'string':\n",
        "    index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
        "  else:\n",
        "    index = preprocessing.IntegerLookup(max_tokens=max_tokens)\n",
        "\n",
        "  # Prepare a Dataset that only yields our feature\n",
        "  feature_ds = dataset.map(lambda x, y: x[name])\n",
        "\n",
        "  # Learn the set of possible values and assign them a fixed integer index.\n",
        "  index.adapt(feature_ds)\n",
        "\n",
        "  # Create a Discretization for our integer indices.\n",
        "  encoder = preprocessing.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
        "\n",
        "  # Apply one-hot encoding to our indices. The lambda function captures the\n",
        "  # layer so we can use them, or include them in the functional model later.\n",
        "  return lambda feature: encoder(index(feature))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2t2ff9K8PcT"
      },
      "outputs": [],
      "source": [
        "type_col = train_features['Type']\n",
        "layer = get_category_encoding_layer('Type', train_ds, 'string')\n",
        "layer(type_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6eDongw8knz"
      },
      "source": [
        "通常，您不应将数字直接输入模型，而是改用这些输入的独热编码。考虑代表宠物年龄的原始数据。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FjBioQ38oNE"
      },
      "outputs": [],
      "source": [
        "type_col = train_features['Age']\n",
        "category_encoding_layer = get_category_encoding_layer('Age', train_ds,\n",
        "                                                      'int64', 5)\n",
        "category_encoding_layer(type_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiE0glOPkMyh"
      },
      "source": [
        "## 选择要使用的列\n",
        "\n",
        "您已经了解了如何使用多种类型的预处理层。现在您将使用它们来训练模型。您将使用 [Keras-functional API](https://tensorflow.google.cn/guide/keras/functional) 来构建模型。Keras 函数式 API 是一种比 [tf.keras.Sequential](https://tensorflow.google.cn/api_docs/python/tf/keras/Sequential) API 更灵活的创建模型的方式。\n",
        "\n",
        "本教程的目标是向您展示使用预处理层所需的完整代码（例如机制）。任意选择了几列来训练我们的模型。\n",
        "\n",
        "要点：如果您的目标是构建一个准确的模型，请尝试使用自己的更大的数据集，并仔细考虑哪些特征最有意义，以及它们应该如何表示。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj1GoHSZ9R3H"
      },
      "source": [
        "之前，您使用了小批次来演示输入流水线。现在让我们创建一个具有更大批次大小的新输入流水线。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rcv2kQTTo23h"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3RBa51VkaAn"
      },
      "outputs": [],
      "source": [
        "all_inputs = []\n",
        "encoded_features = []\n",
        "\n",
        "# Numeric features.\n",
        "for header in ['PhotoAmt', 'Fee']:\n",
        "  numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
        "  normalization_layer = get_normalization_layer(header, train_ds)\n",
        "  encoded_numeric_col = normalization_layer(numeric_col)\n",
        "  all_inputs.append(numeric_col)\n",
        "  encoded_features.append(encoded_numeric_col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FOMGfZflhoA"
      },
      "outputs": [],
      "source": [
        "# Categorical features encoded as integers.\n",
        "age_col = tf.keras.Input(shape=(1,), name='Age', dtype='int64')\n",
        "encoding_layer = get_category_encoding_layer('Age', train_ds, dtype='int64',\n",
        "                                             max_tokens=5)\n",
        "encoded_age_col = encoding_layer(age_col)\n",
        "all_inputs.append(age_col)\n",
        "encoded_features.append(encoded_age_col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8C8xyiXm-Ie"
      },
      "outputs": [],
      "source": [
        "# Categorical features encoded as string.\n",
        "categorical_cols = ['Type', 'Color1', 'Color2', 'Gender', 'MaturitySize',\n",
        "                    'FurLength', 'Vaccinated', 'Sterilized', 'Health', 'Breed1']\n",
        "for header in categorical_cols:\n",
        "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
        "  encoding_layer = get_category_encoding_layer(header, train_ds, dtype='string',\n",
        "                                               max_tokens=5)\n",
        "  encoded_categorical_col = encoding_layer(categorical_col)\n",
        "  all_inputs.append(categorical_col)\n",
        "  encoded_features.append(encoded_categorical_col)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHSnhz2fyEq3"
      },
      "source": [
        "## 创建、编译并训练模型\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDGyN_wpo0XS"
      },
      "source": [
        "接下来，您可以创建端到端模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Yrj-_pr6jyL"
      },
      "outputs": [],
      "source": [
        "all_features = tf.keras.layers.concatenate(encoded_features)\n",
        "x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "output = tf.keras.layers.Dense(1)(x)\n",
        "model = tf.keras.Model(all_inputs, output)\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6mNMfG6yEq5"
      },
      "source": [
        "我们来可视化连接图：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7Bkx4c7yEq5"
      },
      "outputs": [],
      "source": [
        "# rankdir='LR' is used to make the graph horizontal.\n",
        "tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CED6OStLyEq7"
      },
      "source": [
        "### 训练模型。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQfE3PC6yEq8"
      },
      "outputs": [],
      "source": [
        "model.fit(train_ds, epochs=10, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8N2uAdU2Cni"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(\"Accuracy\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmZMnTKaCZda"
      },
      "source": [
        "## 根据新数据进行推断\n",
        "\n",
        "要点：您开发的模型现在可以直接从 CSV 文件中对行进行分类，因为预处理代码包含在模型本身中。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xkOlK8Zweeh"
      },
      "source": [
        "现在，您可以保存并重新加载 Keras 模型。请按照[此处](https://tensorflow.google.cn/tutorials/keras/save_and_load)的教程了解有关 TensorFlow 模型的更多信息。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH9Zy1sBvwOH"
      },
      "outputs": [],
      "source": [
        "model.save('my_pet_classifier')\n",
        "reloaded_model = tf.keras.models.load_model('my_pet_classifier')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D973plJrdwQ9"
      },
      "source": [
        "要获得对新样本的预测，只需调用 `model.predict()`。您只需要做两件事：\n",
        "\n",
        "1. 将标量封装成列表，以便具有批次维度（模型只处理成批次的数据，而不是单个样本）\n",
        "2. 对每个特征调用 `convert_to_tensor`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKq4pxtdDa7i"
      },
      "outputs": [],
      "source": [
        "sample = {\n",
        "    'Type': 'Cat',\n",
        "    'Age': 3,\n",
        "    'Breed1': 'Tabby',\n",
        "    'Gender': 'Male',\n",
        "    'Color1': 'Black',\n",
        "    'Color2': 'White',\n",
        "    'MaturitySize': 'Small',\n",
        "    'FurLength': 'Short',\n",
        "    'Vaccinated': 'No',\n",
        "    'Sterilized': 'No',\n",
        "    'Health': 'Healthy',\n",
        "    'Fee': 100,\n",
        "    'PhotoAmt': 2,\n",
        "}\n",
        "\n",
        "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
        "predictions = reloaded_model.predict(input_dict)\n",
        "prob = tf.nn.sigmoid(predictions[0])\n",
        "\n",
        "print(\n",
        "    \"This particular pet had a %.1f percent probability \"\n",
        "    \"of getting adopted.\" % (100 * prob)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJQQZEiH2FaB"
      },
      "source": [
        "要点：对于更大、更复杂的数据集，您通常会看到深度学习的最佳结果。在处理像这样的小数据集时，我们建议使用决策树或随机森林作为强基线。本教程的目标是演示处理结构化数据的机制，以便您将来处理自己的数据集时有可以作为起点的代码。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0QAY2Tb2HYG"
      },
      "source": [
        "## 后续步骤\n",
        "\n",
        "进一步了解有关结构化数据分类的最佳方法是自己尝试。您可能希望找到另一个可使用的数据集，并使用与上述类似的代码训练模型对其进行分类。为了提高准确率，请仔细考虑要在模型中包含哪些特征，以及它们应该如何表示。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "preprocessing_layers.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
